############## input and output ###############

# Full path to input train dataset
input.trainData=/scratch/li.che/mlc_data_pyramid/rcv1subset_topics_1/train_test_split/train

# Full path to input validation dataset, if available
# Used for hyper parameter tuning and early stopping
# If no additional validation set is available, leave it as blank, 
# and random 20% of the training data will be used as the validation set. 
input.validData=

# Full path to input test dataset
input.testData=/scratch/li.che/mlc_data_pyramid/rcv1subset_topics_1/train_test_split/test

# Directory for the program output
output.dir=/scratch/li.che/out/cbm_lr/rcv1

# Whether to show detailed debugging information
output.verbose=false

################# functions #####################

# Perform hyper parameter tuning before training
# If external validation data is given, the model is trained on the full training data
# and tuned on the given validation data; otherwise, the model is trained on 80% of the training data,
# and tuned on the rest 20% of the training data.
tune=false

# Train the model on all the available data (excluding test data), using tuned or user specified hyper parameters
# If the external validation data is also given, the model is trained on training data + validation data
train=true

# Load back trained model, make predictions on the test set, and evaluate test performance.
# The program shows several different predictions designed to optimize different evaluation metrics.
test=true

######### prediction method ########

# Whether to allow empty subset to be predicted; 
# true = allow empty prediction
# false = do not allow empty prediction
# auto = allow empty prediction only if the training set contains empty label sets
predict.allowEmpty=auto

# The threshold for skipping components with small contributions
# This is designed to speed up prediction
predict.piThreshold=0.001


######### tune #########

# Hyper parameter tuning uses the validation set to decide logistic regression regularization variance, 
# number of CBM components and number of EM training iterations.
# Users can specify candidate values for variance and components.
# The optimal EM training iterations will be determined automatically by monitoring the validation performance.
# The metric monitored is specified in predict.targetMetric.

# To achieve optimal prediction under which target evaluation metric? 
# Currently supported metrics: instance_set_accuracy, instance_f1 and instance_hamming_loss.
# Generally speaking no single model is well suited for all evaluation metrics.
# Optimizing different metrics require different prediction methods and hyper parameters.
# The program automatically chooses the optimal prediction method designed for each metric.
# The predictor designed for instance set accuracy outputs the joint mode.
# The predictor designed for instance Hamming loss outputs the marginal modes.
# The predictor designed for instance F1 runs the GFM algorithm.
# The metric specified here will serve as the main metric for selecting the best model during hyper parameter tuning
# Once the model is trained, the program shows all different predictions made by different prediction methods
tune.targetMetric=instance_set_accuracy

# What values to try for logistic regression Gaussian prior variance (for L2 regularization)
# Small values indicate strong regularizations
# The variance can greatly affect the performance and thus requires careful tuning
tune.variance.candidates=0.1,1,1000,1000000

# What values to try for number of CBM components
# The default value 50 usually gives good performance
# To reduce turning time, users can just set tune.numComponents.candidates=50
tune.numComponents.candidates=1,20,50,100


# Evaluate the metric on the validation set every k iterations
# Frequent evaluation may slow down training
# Use a small value (e.g. 1) if we expect the training to take just a few iterations (e.g. 20)
# Use a big value (e.g. 10) if we expect the training to take many iterations (e.g. 200)
tune.monitorInterval=1

# the model training will never stop before it reaches this minimum number of iterations
tune.earlyStop.minIterations=5

# If the validation metric does not improve after k successive evaluations, the training will stop
# for example, if tune.monitorInterval=5 and tune.earlyStop.patience=2, trains stops if no improvement in 10 iterations
# Using a patient value too small make cause the training to stop too early
# Using a patient value too big make increase the tuning time
tune.earlyStop.patience=10


######### train #################

# Whether to use optimal hyper parameter values found by tuning
# These hyper parameters include: train.iterations, train.variance, and train.numComponents
# if true, users do not need to specify these values
# if false or if no tuning has be performed, users need to provide a value for each of them 
train.useTunedHyperParameters=false

# Number of EM training iterations
train.iterations=4

# Logistic regression Gaussian prior variance (for L2 regularization)
# The variance can greatly affect the performance and thus requires careful tuning
# The default value 1 may not be optimal
train.variance=1000000

# Number of CBM components
# The default value 50 usually gives good performance
train.numComponents=50


# The parameters below usually do not affect the performance much
# Users can use default values

# number of LBFGS parameter updates for LR in each M step
# The default value 10 is good most of the time
# If the train.iterations found by hyper parameter tuning is 1 or 2, each M step is probably doing too much work and the training overfits too quickly. In this case, we can decrease train.updatesPerIteration
train.updatesPerIteration=10

# In each component, skip instances with small memberships values (gammas)
# This is designed to speed up training
train.skipDataThreshold=0.00001

# Skip training a classifier for a label in a component if that label almost never appears or almost always appears in that component. 
# A constant output (the prior probability) will be used in this case.
# This is designed to speed up training
train.skipLabelThreshold=0.00001
# Smooth the probability of a non-existent label in a component with the its overall probability in the dataset
# This is designed to avoid zero probabilities
train.smoothStrength=0.0001

######## test ##############

# When generating prediction reports for individual label probabilities, labels with probabilities below the threshold will not be displayed
# This only make the reports more readable; it does not affect the actual prediction in any way.
report.labelProbThreshold=0.2





# the internal Java class name for this application. 
# users do not need to modify this.
pyramid.class=CBMLR
