########## IO ###########
# folder for all outputs
output.folder=/huge1/people/chengli/projects/pyramid/archives/app3/ohsumed_20000/1
# the name of the train folder
output.trainFolder=train
# the name of the test folder
output.testFolder=test

######### functions ##########
# generate training set
createTrainSet=true
# generate test set
createTestSet=true
# train model; produce reports for training set; train should be run prior to test
train=true
# after "train", run "tune" to search for the best threshold for the target evaluation measure
# tuning is necessary if predict.target=macroFMeasure
tune=true
# load model; produce reports for test set
test=true

######### feature ########## 
feature.useInitialFeatures=true
feature.categFeature.filter=true
feature.categFeature.percentThreshold=0.1
feature.ngram.n=1,2
feature.ngram.minDf=10
feature.ngram.slop=0,1
feature.missingValue=false
# generate feature distribution in order to analyze top features
feature.generateDistribution=false

# whether to add external ngrams from a file
# if true, user specified ngrams will be added to the feature matrix
feature.addExternalNgrams=true
# the file containing user specified ngrams, one ngram per line
# users do not need to manually analyze (tokenize, stem, filter) these ngrams, but need to specify the right analyzer below
feature.externalNgramFile=/home/chengli/experiments/pyramid/application/app3/configs/ohsumed_20000/ngramlist

# whether to filter ngram candidates based on a list of unigram keywords 
# if true, only ngrams (n>1) containing at least one of the keywords will be kept
feature.filterNgramsByKeyWords=true
# the file containing unigram keywords, one unigram per line
# users do not need to manually analyze (tokenize, stem, filter) these unigrams, but need to specify the right analyzer below
feature.filterNgrams.keyWordsFile=/home/chengli/experiments/pyramid/application/app3/configs/ohsumed_20000/keywords

# the program will use this analyzer to analyze (tokenize, stem, filter) the user provided ngrams and unigram keywords
# this analyzer should be the same as the one specified in the Elasticsearch body field mapping
feature.analyzer=english

# whether to filter ngram candidates by a regular expression
# if true, ngrams matching the regular expression will be removed
feature.filterNgramsByRegex=false

# the regular expression used to filter ngrams
# for example, \\d will remove a unigram if it is a digit
# users can specify ngram length in the regular expression using the number of spaces (an ngram has n-1 spaces)
# to help users understand and test the regular expression matching function, pyramid provides another application called "regex"
# users are encouraged to test their regular expression first with "regex"
feature.filterNgrams.regex=\\d

########## index ########## 
index.indexName=ohsumed_20000
index.clusterName=fijielasticsearch
index.documentType=document
# node or transport
index.clientType=node
# set hosts if clientType=transport
index.hosts=fiji11,fiji12
# set ports if clientType=transport
index.ports=9300,9300
index.labelField=real_labels
index.labelFilter=false
index.labelFilter.prefix=foo
index.featureFieldPrefix=feature
index.ngramExtractionFields=body

# can be es_original, frequency, binary, tfifl
# tfifl = term frequency normalized by field length;
# to use tfifl, users should manually store the field length in a separate field named <field_name>_field_length, e.g., body_field_length
index.ngramMatchScoreType=es_original

# can be field or query
# if index.splitMode=field, the split is made based on index.splitField, index.splitField.train and index.splitField.test
# if index.splitMode=query, the split is made based on splitQuery.train and index.splitQuery.test
index.splitMode=field

# if splitBy=field
# field which stores train vs test
index.splitField=split
# field value for training documents
index.splitField.train=train
# field value for test documents
index.splitField.test=test

# if splitBy=query
# the query string should be the string after the top level "query":
# For example, if the curl command is
# curl -XGET "http://localhost:9200/ohsumed_20000/document/_search" -d'{"query":{"filtered":{"query":{"match_all":{}},"filter":{"term":{"split":"train"}}}}}'
# then the query string should be {"filtered":{"query":{"match_all":{}},"filter":{"term":{"split":"train"}}}}
# make sure the curly braces match

# the elasticsearch query string for matching training documents
index.splitQuery.train={"filtered":{"query":{"match_all":{}},"filter":{"term":{"split":"train"}}}}
# the elasticsearch query string for matching test documents
index.splitQuery.test={"filtered":{"query":{"match_all":{}},"filter":{"term":{"split":"test"}}}}

############# train ##########
# start with current model; train for more iterations
train.warmStart=false
train.usePrior=true
train.numIterations=100
train.numLeaves=5
train.learningRate=0.1
train.minDataPerLeaf=3
train.numSplitIntervals=100
train.showTrainProgress=true
train.showTestProgress=false
train.showProgress.interval=1
train.generateReports=true

############ tune ###########
# whether to use train or test data to tune the threshold
tune.data=train

# The F-measure was derived so that F_\beta "measures the effectiveness of retrieval with respect to a user who attaches Î² times as much importance to recall as precision"
# Two other commonly used F measures are the F_{2} measure, which weights recall higher than precision, 
# and the F_{0.5} measure, which puts more emphasis on precision than recall. --wikipedia
tune.FMeasure.beta=1

############ predict ##############
# to achieve optimal prediction under which target measure 
# subsetAccuracy, hammingLoss, instanceFMeasure, macroFMeasure
# if prediction.target=macroFMeasure, user should run "tune" after "train"
predict.target=subsetAccuracy

############ report ##################
report.topFeatures.limit=10
report.rule.limit=10
report.numDocsPerFile=100
report.classProbThreshold=0.4
report.labelSetLimit=10